{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "In this notebook, we showcase how to fine-tune the Qwen3-1.7B model on AWS Trainium using the Hugging Face Optimum Neuron library.\n",
    "The goal of this task is Text-to-SQL generation â€” training the model to translate natural language questions into executable SQL queries.\n",
    "\n",
    "We will fine-tune the model using `optimum.neuron`, save the trained checkpoint, and then deploy it for inference with Optimum-Neuron[vllm], enabling high-performance, low-latency Text-to-SQL execution.\n",
    "\n",
    "By the end of this notebook, youâ€™ll have a fine-tuned, Trainium-optimized Qwen3 model ready for deployment and real-time inference. This workflow demonstrates how to leverage the Optimum Neuron toolchain to efficiently train and serve large language models on AWS Neuron devices.\n",
    "\n",
    "For this module, you will be using the [b-mc2/sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) dataset which consists of thousands of examples of SQL schemas, questions about the schemas, and SQL queries intended to answer the questions.\n",
    "\n",
    "*Dataset example 1:*\n",
    "* *SQL schema/context:* `CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)`\n",
    "* *Question:* `How many departments are led by heads who are not mentioned?`\n",
    "* *SQL query/answer:* `SELECT COUNT(*) FROM department WHERE NOT department_id IN (SELECT department_id FROM management)`\n",
    "\n",
    "*Dataset example 2:*\n",
    "* *SQL schema/context:* `CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR, course_id VARCHAR)`\n",
    "* *Question:* `What are the ids of all students for courses and what are the names of those courses?`\n",
    "* *SQL query/answer:* `SELECT T1.student_id, T2.course_name FROM student_course_registrations AS T1 JOIN courses AS T2 ON T1.course_id = T2.course_id`\n",
    "\n",
    "By fine-tuning the model over several thousand of these text-to-SQL examples, the model will then learn how to generate an appropriate SQL query when presented with a SQL context and a free-form question.\n",
    "\n",
    "This text-to-SQL use case was selected so you can successfully fine-tune your model in a reasonably short amount of time (~25 minutes) which is appropriate for this workshop. Although this is a relatively simple use case, please keep in mind that the same techniques and components used in this module can also be applied to fine-tune LLMs for more advanced use cases such as writing code, summarizing documents, creating blog posts - the possibilities are endless!\n",
    "\n",
    "# Install requirements\n",
    "This notebook uses [Hugging Face Optimum Neuron](https://github.com/huggingface/optimum-neuron) which works like an interface between the Hugging Face Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. We will also install some other libraries like peft, trl etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/environment/FineTuning/HuggingFaceExample/01_finetuning/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting optimum-neuron==0.3.0 (from -r requirements.txt (line 1))\n",
      "  Using cached optimum_neuron-0.3.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting peft==0.16.0 (from -r requirements.txt (line 2))\n",
      "  Using cached peft-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting trl==0.11.4 (from -r requirements.txt (line 3))\n",
      "  Using cached trl-0.11.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting huggingface_hub==0.33.4 (from -r requirements.txt (line 4))\n",
      "  Using cached huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting datasets==3.6.0 (from -r requirements.txt (line 5))\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers~=4.51.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from optimum-neuron==0.3.0->-r requirements.txt (line 1)) (4.51.3)\n",
      "Collecting accelerate==1.8.1 (from optimum-neuron==0.3.0->-r requirements.txt (line 1))\n",
      "  Using cached accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting optimum~=1.24.0 (from optimum-neuron==0.3.0->-r requirements.txt (line 1))\n",
      "  Using cached optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting numpy<=1.25.2,>=1.22.2 (from optimum-neuron==0.3.0->-r requirements.txt (line 1))\n",
      "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting protobuf<4,>=3.20.3 (from optimum-neuron==0.3.0->-r requirements.txt (line 1))\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from peft==0.16.0->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from peft==0.16.0->-r requirements.txt (line 2)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from peft==0.16.0->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from peft==0.16.0->-r requirements.txt (line 2)) (2.7.1)\n",
      "Requirement already satisfied: tqdm in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from peft==0.16.0->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from peft==0.16.0->-r requirements.txt (line 2)) (0.5.3)\n",
      "Collecting tyro>=0.5.11 (from trl==0.11.4->-r requirements.txt (line 3))\n",
      "  Using cached tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from huggingface_hub==0.33.4->-r requirements.txt (line 4)) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from huggingface_hub==0.33.4->-r requirements.txt (line 4)) (2025.7.0)\n",
      "Requirement already satisfied: requests in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from huggingface_hub==0.33.4->-r requirements.txt (line 4)) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from huggingface_hub==0.33.4->-r requirements.txt (line 4)) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from huggingface_hub==0.33.4->-r requirements.txt (line 4)) (1.1.5)\n",
      "Collecting pyarrow>=15.0.0 (from datasets==3.6.0->-r requirements.txt (line 5))\n",
      "  Using cached pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.6.0->-r requirements.txt (line 5))\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from datasets==3.6.0->-r requirements.txt (line 5)) (2.3.1)\n",
      "Collecting xxhash (from datasets==3.6.0->-r requirements.txt (line 5))\n",
      "  Using cached xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets==3.6.0->-r requirements.txt (line 5))\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub==0.33.4->-r requirements.txt (line 4))\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (3.12.15)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from transformers~=4.51.0->optimum-neuron==0.3.0->-r requirements.txt (line 1)) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from transformers~=4.51.0->optimum-neuron==0.3.0->-r requirements.txt (line 1)) (0.21.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from requests->huggingface_hub==0.33.4->-r requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from requests->huggingface_hub==0.33.4->-r requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from requests->huggingface_hub==0.33.4->-r requirements.txt (line 4)) (2025.7.14)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from triton==3.3.1->torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (1.3.0)\n",
      "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3))\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3)) (14.1.0)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3))\n",
      "  Using cached shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3))\n",
      "  Using cached typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.4->-r requirements.txt (line 3)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.16.0->-r requirements.txt (line 2)) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from pandas->datasets==3.6.0->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from pandas->datasets==3.6.0->-r requirements.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from pandas->datasets==3.6.0->-r requirements.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0->-r requirements.txt (line 5)) (1.17.0)\n",
      "Using cached optimum_neuron-0.3.0-py3-none-any.whl (470 kB)\n",
      "Using cached peft-0.16.0-py3-none-any.whl (472 kB)\n",
      "Using cached trl-0.11.4-py3-none-any.whl (316 kB)\n",
      "Using cached huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached optimum-1.24.0-py3-none-any.whl (433 kB)\n",
      "Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Using cached pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (47.6 MB)\n",
      "Using cached tyro-0.9.35-py3-none-any.whl (132 kB)\n",
      "Using cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Using cached shtab-1.8.0-py3-none-any.whl (14 kB)\n",
      "Using cached typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Using cached xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: xxhash, typeguard, shtab, pyarrow, protobuf, numpy, fsspec, docstring-parser, dill, multiprocess, huggingface_hub, tyro, accelerate, peft, optimum, datasets, trl, optimum-neuron\n",
      "\u001b[2K  Attempting uninstall: protobufmâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: protobuf 6.31.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling protobuf-6.31.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled protobuf-6.31.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K  Attempting uninstall: numpym\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/18\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/18\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/18\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/18\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: fsspec[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.7.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling fsspec-2025.7.0:mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.7.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: dillm\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: dill 0.4.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling dill-0.4.0:m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled dill-0.4.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/18\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: huggingface_hub\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/18\u001b[0m [dill]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.34.3â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/18\u001b[0m [dill]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.34.3:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/18\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.34.3â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/18\u001b[0m [dill]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18/18\u001b[0m [optimum-neuron]m [optimum-neuron]]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.8.1 datasets-3.6.0 dill-0.3.8 docstring-parser-0.17.0 fsspec-2025.3.0 huggingface_hub-0.33.4 multiprocess-0.70.16 numpy-1.25.2 optimum-1.24.0 optimum-neuron-0.3.0 peft-0.16.0 protobuf-3.20.3 pyarrow-22.0.0 shtab-1.8.0 trl-0.11.4 typeguard-4.4.4 tyro-0.9.35 xxhash-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/environment/FineTuning/HuggingFaceExample/01_finetuning/assets\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "\n",
    "In this section, we fine-tune the Qwen3-1.7B model on the Text-to-SQL task using Hugging Face Optimum Neuron. Here are the parameters we are going to pass - \n",
    "\n",
    "1. `--nnodes`:\tNumber of nodes (1 = single node)\n",
    "2. `--nproc_per_node`: \tProcesses per node (usually equals number of devices).\n",
    "3. `--model_id, --tokenizer_id`:\tModel and tokenizer identifiers (from Hugging Face or local path).\n",
    "4. `--output_dir`:\tDirectory for saving checkpoints and logs.\n",
    "5. `--bf16`:\tEnables bfloat16 precision for faster, memory-efficient training.\n",
    "5. `--gradient_checkpointing`:\tSaves memory by recomputing activations during backprop.\n",
    "6. `--gradient_accumulation_steps`:\tSteps to accumulate gradients before optimizer update.\n",
    "7. `--learning_rate`:\tInitial training learning rate.\n",
    "8. `--max_steps`:\tTotal number of training steps.\n",
    "9. `--per_device_train_batch_size`:\tBatch size per device.\n",
    "10. `--tensor_parallel_size`:\tNumber of devices for tensor parallelism.\n",
    "11. `--lora_r, --lora_alpha, --lora_dropout`:\tLoRA hyperparameters â€” rank, scaling, and dropout rate.\n",
    "12. `--dataloader_drop_last`:\tDrops last incomplete batch.\n",
    "13. `--disable_tqdm`: Disables progress bar.\n",
    "14. `--logging_steps`:\tLog interval (in steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1202 20:30:09.610000 34836 torch/distributed/run.py:766] \n",
      "W1202 20:30:09.610000 34836 torch/distributed/run.py:766] *****************************************\n",
      "W1202 20:30:09.610000 34836 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1202 20:30:09.610000 34836 torch/distributed/run.py:766] *****************************************\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "2025-12-02 20:30:27.672460: W neuron/nrt_adaptor.cc:53] nrt_tensor_write_hugepage() is not available, will fall back to nrt_tensor_write().\n",
      "2025-12-02 20:30:27.672460: W neuron/nrt_adaptor.cc:53] nrt_tensor_write_hugepage() is not available, will fall back to nrt_tensor_write().\n",
      "2025-12-02 20:30:27.672496: W neuron/nrt_adaptor.cc:62] nrt_tensor_read_hugepage() is not available, will fall back to nrt_tensor_read().\n",
      "2025-12-02 20:30:27.672497: W neuron/nrt_adaptor.cc:62] nrt_tensor_read_hugepage() is not available, will fall back to nrt_tensor_read().\n",
      "2025-Dec-02 20:30:27.0681 34852:34915 [1] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):213 CCOM WARN NET/OFI Failed to initialize sendrecv protocol\n",
      "2025-Dec-02 20:30:27.0681 34851:34916 [0] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):213 CCOM WARN NET/OFI Failed to initialize sendrecv protocol\n",
      "2025-Dec-02 20:30:27.0683 34852:34915 [1] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):354 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2025-Dec-02 20:30:27.0685 34851:34916 [0] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):354 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2025-Dec-02 20:30:27.0687 34852:34915 [1] ncclResult_t nccl_net_ofi_init_no_atexit_fini_v6(ncclDebugLogger_t):183 CCOM WARN NET/OFI Initializing plugin failed\n",
      "2025-Dec-02 20:30:27.0689 34851:34916 [0] ncclResult_t nccl_net_ofi_init_no_atexit_fini_v6(ncclDebugLogger_t):183 CCOM WARN NET/OFI Initializing plugin failed\n",
      "2025-Dec-02 20:30:27.0691 34852:34915 [1] net_plugin.cc:97 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "2025-Dec-02 20:30:27.0693 34851:34916 [0] net_plugin.cc:97 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "[2025-12-02 20:30:27.888: I neuronx_distributed/parallel_layers/parallel_state.py:628] > initializing tensor model parallel with size 2\n",
      "[2025-12-02 20:30:27.889: I neuronx_distributed/parallel_layers/parallel_state.py:629] > initializing pipeline model parallel with size 1\n",
      "[2025-12-02 20:30:27.889: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing context model parallel with size 1\n",
      "[2025-12-02 20:30:27.889: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing data parallel with size 1\n",
      "[2025-12-02 20:30:27.889: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing world size to 2\n",
      "2025-12-02 20:30:27.000977:  34851  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/ubuntu/neuroncc_compile_workdir/ebba86a9-8448-459e-91be-00c4f6136bfc/model.MODULE_910320559639473753+e30acd3a.hlo_module.pb --output /tmp/ubuntu/neuroncc_compile_workdir/ebba86a9-8448-459e-91be-00c4f6136bfc/model.MODULE_910320559639473753+e30acd3a.neff --target=trn1 --verbose=35\n",
      ".Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "[2025-12-02 20:30:31.250: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x788eebab1630>, 'Ascending Ring PG Group')>\n",
      "[2025-12-02 20:30:31.251: I neuronx_distributed/parallel_layers/parallel_state.py:668] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-12-02 20:30:31.251: I neuronx_distributed/parallel_layers/parallel_state.py:669] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-12-02 20:30:31.251: I neuronx_distributed/parallel_layers/parallel_state.py:670] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-12-02 20:30:31.251: I neuronx_distributed/parallel_layers/parallel_state.py:671] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-12-02 20:30:31.251: I neuronx_distributed/parallel_layers/parallel_state.py:672] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-12-02 20:30:31.251: I neuronx_distributed/parallel_layers/parallel_state.py:673] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "No Hugging Face token found in environment, checking AWS Secrets Manager...\n",
      "Logging in to Hugging Face Hub...\n",
      "README.md: 4.43kB [00:00, 23.1MB/s]\n",
      "2025-12-02 20:30:32.000979:  34852  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_910320559639473753+e30acd3a/model.neff\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "No Hugging Face token found in environment, checking AWS Secrets Manager...\n",
      "Logging in to Hugging Face Hub...\n",
      "sql_create_context_v4.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.8M/21.8M [00:01<00:00, 19.7MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆ| 78577/78577 [00:00<00:00, 386223.18 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 15372.58 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 14465.41 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:03<00:00, 14835.42 examples/s]\n",
      "tokenizer_config.json: 9.73kB [00:00, 58.4MB/s]\n",
      "vocab.json: 2.78MB [00:00, 95.2MB/s]\n",
      "merges.txt: 1.67MB [00:00, 172MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.4M/11.4M [00:00<00:00, 27.2MB/s]\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 726/726 [00:00<00:00, 7.46MB/s]\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/kvcache/kv_cache_manager.py:24: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..attention.gqa import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/kvcache/kv_cache_manager.py:24: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..attention.gqa import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/models/inference/llama/modeling_llama.py:45: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/models/inference/llama/modeling_llama.py:45: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "model.safetensors.index.json: 25.6kB [00:00, 141MB/s]\n",
      "Fetching 2 files:   0%|                                   | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/3.44G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   0%|              | 0.00/622M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   0%|    | 68.9k/622M [00:00<1:32:20, 112kB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   3%|â–    | 18.9M/622M [00:00<00:19, 30.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  14%|â–Š     | 86.0M/622M [00:01<00:05, 107MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|    | 18.8M/3.44G [00:01<03:27, 16.5MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–   | 287M/622M [00:01<00:00, 380MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 421M/622M [00:01<00:00, 522MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|    | 77.1M/3.44G [00:01<00:59, 56.3MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|â–     | 218M/3.44G [00:01<00:16, 191MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|â–     | 286M/3.44G [00:01<00:12, 251MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|â–‹     | 364M/3.44G [00:01<00:10, 303MB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 555M/622M [00:02<00:00, 312MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622M/622M [00:02<00:00, 286MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  15%|â–‰     | 514M/3.44G [00:02<00:07, 380MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|â–ˆ     | 616M/3.44G [00:02<00:07, 369MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|â–ˆâ–    | 686M/3.44G [00:02<00:08, 324MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|â–ˆâ–    | 763M/3.44G [00:02<00:06, 384MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|â–ˆâ–    | 838M/3.44G [00:03<00:06, 417MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|â–ˆâ–Œ    | 888M/3.44G [00:03<00:07, 353MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|â–ˆâ–‹    | 940M/3.44G [00:03<00:09, 256MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|â–ˆâ–‹    | 979M/3.44G [00:03<00:10, 238MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|â–ˆâ–Œ   | 1.05G/3.44G [00:04<00:08, 267MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|â–ˆâ–‹   | 1.18G/3.44G [00:04<00:06, 327MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|â–ˆâ–Š   | 1.24G/3.44G [00:04<00:06, 321MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|â–ˆâ–‰   | 1.34G/3.44G [00:05<00:07, 275MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|â–ˆâ–‰   | 1.37G/3.44G [00:05<00:07, 285MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|â–ˆâ–ˆ   | 1.44G/3.44G [00:05<00:11, 175MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|â–ˆâ–ˆâ–  | 1.50G/3.44G [00:06<00:10, 187MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|â–ˆâ–ˆâ–  | 1.57G/3.44G [00:06<00:10, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|â–ˆâ–ˆâ–  | 1.59G/3.44G [00:06<00:10, 174MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|â–ˆâ–‰  | 1.66G/3.44G [00:08<00:19, 89.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|â–ˆâ–ˆ  | 1.72G/3.44G [00:08<00:17, 96.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|â–ˆâ–ˆ  | 1.75G/3.44G [00:09<00:19, 87.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|â–ˆâ–ˆ  | 1.82G/3.44G [00:11<00:29, 55.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|â–ˆâ–ˆâ– | 1.89G/3.44G [00:11<00:20, 75.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|â–ˆâ–ˆâ– | 1.92G/3.44G [00:13<00:30, 50.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|â–ˆâ–ˆâ– | 1.97G/3.44G [00:13<00:28, 52.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|â–ˆâ–ˆâ– | 2.08G/3.44G [00:14<00:19, 70.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|â–ˆâ–ˆâ– | 2.13G/3.44G [00:15<00:18, 70.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|â–ˆâ–ˆâ–Œ | 2.19G/3.44G [00:16<00:17, 69.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|â–ˆâ–ˆâ–Œ | 2.25G/3.44G [00:17<00:16, 73.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|â–ˆâ–ˆâ–‹ | 2.31G/3.44G [00:18<00:18, 60.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|â–ˆâ–ˆâ–‹ | 2.34G/3.44G [00:18<00:17, 64.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|â–ˆâ–ˆâ–Š | 2.37G/3.44G [00:19<00:15, 69.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|â–ˆâ–ˆâ–Š | 2.43G/3.44G [00:19<00:11, 90.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–‹ | 2.55G/3.44G [00:19<00:05, 172MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–Š | 2.60G/3.44G [00:20<00:05, 156MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–‰ | 2.68G/3.44G [00:20<00:05, 152MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆ | 2.82G/3.44G [00:21<00:03, 193MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 2.98G/3.44G [00:21<00:01, 252MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.02G/3.44G [00:22<00:02, 192MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–| 3.08G/3.44G [00:22<00:02, 154MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.14G/3.44G [00:23<00:02, 132MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.21G/3.44G [00:24<00:01, 118MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.27G/3.44G [00:24<00:01, 113MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.44G/3.44G [00:25<00:00, 136MB/s]\u001b[A\n",
      "Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:25<00:00, 12.76s/it]\n",
      "Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:25<00:00, 12.76s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.01s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.26s/it]\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/transformers/training_args.py:2058: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/transformers/training_args.py:2058: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "Generating train split: 5532 examples [00:12, 453.99 examples/s]\n",
      "Generating train split: 55 examples [00:00, 367.16 examples/s]\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/trainers.py:1467: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `NeuronSFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer.__init__(self, *args, **kwargs)\n",
      "No label_names provided for model class `NeuronPeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/trainers.py:1726: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/trainers.py:1467: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `NeuronSFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer.__init__(self, *args, **kwargs)\n",
      "No label_names provided for model class `NeuronPeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/trainers.py:1726: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5,532\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Total optimization steps = 1,000\n",
      "  Number of trainable parameters = 16,515,072\n",
      "model.neff: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.3k/11.3k [00:00<00:00, 86.7MB/s]\n",
      "2025-12-02 20:31:34.000476:  34851  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_14152517080381013961+a3455b04/model.neff\n",
      "2025-12-02 20:31:34.000496:  34852  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_14152517080381013961+a3455b04/model.neff\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_14(â€¦): 100%|â–ˆ| 21.4M/21.4M [00:00<00:00, 2\n",
      "2025-12-02 20:31:38.000368:  34852  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_14770574952780443587+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_13(â€¦): 100%|â–ˆ| 21.4M/21.4M [00:00<00:00, 2\n",
      "2025-12-02 20:31:38.000412:  34851  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_13340102934497073113+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_44(â€¦): 100%|â–ˆ| 21.3M/21.3M [00:00<00:00, 4\n",
      "2025-12-02 20:31:44.000708:  34852  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_4493465029485591167+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_63(â€¦): 100%|â–ˆ| 21.4M/21.4M [00:00<00:00, 4\n",
      "2025-12-02 20:31:44.000717:  34851  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_6394509931854585423+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_53(â€¦): 100%|â–ˆ| 16.4M/16.4M [00:00<00:00, 3\n",
      "2025-12-02 20:31:54.000324:  34852  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_5391683354399705816+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_17(â€¦): 100%|â–ˆ| 15.6M/15.6M [00:00<00:00, 3\n",
      "2025-12-02 20:31:54.000368:  34851  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_17239445601988651504+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_31(â€¦): 100%|â–ˆ| 1.51M/1.51M [00:00<00:00, 3\n",
      "2025-12-02 20:32:01.000489:  34851  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_3188978248012181906+a3455b04/model.neff\n",
      "2025-12-02 20:32:01.000508:  34852  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_3188978248012181906+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_91(â€¦): 100%|â–ˆ| 1.98M/1.98M [00:00<00:00, 5\n",
      "2025-12-02 20:32:05.000350:  34851  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_9123409164792130896+a3455b04/model.neff\n",
      "2025-12-02 20:32:05.000382:  34852  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_9123409164792130896+a3455b04/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_96(â€¦): 100%|â–ˆ| 1.98M/1.98M [00:00<00:00, 5\n",
      "2025-12-02 20:32:15.000934:  34852  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_9654582282420716943+a3455b04/model.neff\n",
      "2025-12-02 20:32:15.000960:  34851  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_9654582282420716943+a3455b04/model.neff\n",
      "{'loss': 1.7589, 'learning_rate': 4.9500000000000004e-05, 'grad_norm': 1.375, 'epoch': 0.0036153289949385392}\n",
      "{'loss': 1.1141, 'learning_rate': 4.9e-05, 'grad_norm': 0.55078125, 'epoch': 0.0072306579898770785}\n",
      "{'loss': 0.9875, 'learning_rate': 4.85e-05, 'grad_norm': 0.41796875, 'epoch': 0.010845986984815618}\n",
      "{'loss': 0.8968, 'learning_rate': 4.8e-05, 'grad_norm': 0.376953125, 'epoch': 0.014461315979754157}\n",
      "{'loss': 0.8117, 'learning_rate': 4.75e-05, 'grad_norm': 0.33984375, 'epoch': 0.018076644974692697}\n",
      "{'loss': 0.8036, 'learning_rate': 4.7e-05, 'grad_norm': 0.455078125, 'epoch': 0.021691973969631236}\n",
      "{'loss': 0.7604, 'learning_rate': 4.6500000000000005e-05, 'grad_norm': 0.4140625, 'epoch': 0.025307302964569775}\n",
      "{'loss': 0.7523, 'learning_rate': 4.600000000000001e-05, 'grad_norm': 0.44140625, 'epoch': 0.028922631959508314}\n",
      "{'loss': 0.734, 'learning_rate': 4.55e-05, 'grad_norm': 0.48046875, 'epoch': 0.03253796095444685}\n",
      "{'loss': 0.6802, 'learning_rate': 4.5e-05, 'grad_norm': 0.4765625, 'epoch': 0.036153289949385395}\n",
      "{'loss': 0.69, 'learning_rate': 4.4500000000000004e-05, 'grad_norm': 0.49609375, 'epoch': 0.03976861894432393}\n",
      "{'loss': 0.6929, 'learning_rate': 4.4000000000000006e-05, 'grad_norm': 0.5, 'epoch': 0.04338394793926247}\n",
      "{'loss': 0.6719, 'learning_rate': 4.35e-05, 'grad_norm': 0.470703125, 'epoch': 0.046999276934201015}\n",
      "{'loss': 0.6586, 'learning_rate': 4.3e-05, 'grad_norm': 0.46484375, 'epoch': 0.05061460592913955}\n",
      "{'loss': 0.6836, 'learning_rate': 4.25e-05, 'grad_norm': 0.53125, 'epoch': 0.05422993492407809}\n",
      "{'loss': 0.7023, 'learning_rate': 4.2e-05, 'grad_norm': 0.5546875, 'epoch': 0.05784526391901663}\n",
      "{'loss': 0.6744, 'learning_rate': 4.15e-05, 'grad_norm': 0.4765625, 'epoch': 0.06146059291395517}\n",
      "{'loss': 0.6754, 'learning_rate': 4.1e-05, 'grad_norm': 0.49609375, 'epoch': 0.0650759219088937}\n",
      "{'loss': 0.6554, 'learning_rate': 4.05e-05, 'grad_norm': 0.478515625, 'epoch': 0.06869125090383225}\n",
      "{'loss': 0.6459, 'learning_rate': 4e-05, 'grad_norm': 0.484375, 'epoch': 0.07230657989877079}\n",
      "{'loss': 0.6305, 'learning_rate': 3.9500000000000005e-05, 'grad_norm': 0.55859375, 'epoch': 0.07592190889370933}\n",
      "{'loss': 0.6645, 'learning_rate': 3.9000000000000006e-05, 'grad_norm': 0.80859375, 'epoch': 0.07953723788864786}\n",
      "{'loss': 0.6399, 'learning_rate': 3.85e-05, 'grad_norm': 0.5, 'epoch': 0.08315256688358641}\n",
      "{'loss': 0.6122, 'learning_rate': 3.8e-05, 'grad_norm': 0.55859375, 'epoch': 0.08676789587852494}\n",
      "{'loss': 0.6024, 'learning_rate': 3.7500000000000003e-05, 'grad_norm': 0.52734375, 'epoch': 0.09038322487346348}\n",
      "{'loss': 0.6132, 'learning_rate': 3.7e-05, 'grad_norm': 0.546875, 'epoch': 0.09399855386840203}\n",
      "{'loss': 0.6194, 'learning_rate': 3.65e-05, 'grad_norm': 0.5, 'epoch': 0.09761388286334056}\n",
      "{'loss': 0.6223, 'learning_rate': 3.6e-05, 'grad_norm': 0.55859375, 'epoch': 0.1012292118582791}\n",
      "{'loss': 0.6218, 'learning_rate': 3.55e-05, 'grad_norm': 0.5859375, 'epoch': 0.10484454085321765}\n",
      "{'loss': 0.6482, 'learning_rate': 3.5e-05, 'grad_norm': 0.53125, 'epoch': 0.10845986984815618}\n",
      "{'loss': 0.598, 'learning_rate': 3.45e-05, 'grad_norm': 0.5234375, 'epoch': 0.11207519884309472}\n",
      "{'loss': 0.6297, 'learning_rate': 3.4000000000000007e-05, 'grad_norm': 0.62109375, 'epoch': 0.11569052783803326}\n",
      "{'loss': 0.607, 'learning_rate': 3.35e-05, 'grad_norm': 0.56640625, 'epoch': 0.1193058568329718}\n",
      "{'loss': 0.6258, 'learning_rate': 3.3e-05, 'grad_norm': 0.5546875, 'epoch': 0.12292118582791034}\n",
      "{'loss': 0.6156, 'learning_rate': 3.2500000000000004e-05, 'grad_norm': 0.63671875, 'epoch': 0.1265365148228489}\n",
      "{'loss': 0.6157, 'learning_rate': 3.2000000000000005e-05, 'grad_norm': 0.5546875, 'epoch': 0.1301518438177874}\n",
      "{'loss': 0.6201, 'learning_rate': 3.15e-05, 'grad_norm': 0.4921875, 'epoch': 0.13376717281272596}\n",
      "{'loss': 0.5866, 'learning_rate': 3.1e-05, 'grad_norm': 0.7265625, 'epoch': 0.1373825018076645}\n",
      "{'loss': 0.5913, 'learning_rate': 3.05e-05, 'grad_norm': 0.59765625, 'epoch': 0.14099783080260303}\n",
      "{'loss': 0.5931, 'learning_rate': 3e-05, 'grad_norm': 0.62890625, 'epoch': 0.14461315979754158}\n",
      "{'loss': 0.6094, 'learning_rate': 2.95e-05, 'grad_norm': 0.640625, 'epoch': 0.14822848879248013}\n",
      "{'loss': 0.6037, 'learning_rate': 2.9e-05, 'grad_norm': 0.57421875, 'epoch': 0.15184381778741865}\n",
      "{'loss': 0.6133, 'learning_rate': 2.8499999999999998e-05, 'grad_norm': 0.5859375, 'epoch': 0.1554591467823572}\n",
      "{'loss': 0.5938, 'learning_rate': 2.8000000000000003e-05, 'grad_norm': 0.59375, 'epoch': 0.15907447577729572}\n",
      "{'loss': 0.6115, 'learning_rate': 2.7500000000000004e-05, 'grad_norm': 0.69140625, 'epoch': 0.16268980477223427}\n",
      "{'loss': 0.5956, 'learning_rate': 2.7000000000000002e-05, 'grad_norm': 0.6640625, 'epoch': 0.16630513376717282}\n",
      "{'loss': 0.5798, 'learning_rate': 2.6500000000000004e-05, 'grad_norm': 0.62109375, 'epoch': 0.16992046276211134}\n",
      "{'loss': 0.6057, 'learning_rate': 2.6000000000000002e-05, 'grad_norm': 0.74609375, 'epoch': 0.1735357917570499}\n",
      "{'loss': 0.6069, 'learning_rate': 2.5500000000000003e-05, 'grad_norm': 0.59375, 'epoch': 0.17715112075198844}\n",
      "{'loss': 0.5835, 'learning_rate': 2.5e-05, 'grad_norm': 0.66015625, 'epoch': 0.18076644974692696}\n",
      "Saving model checkpoint to /home/ubuntu/environment/ml/qwen/checkpoint-500\n",
      "Model parallelism is enabled, saving the model sharded state dict instead of the full state dict.\n",
      "[2025-12-02 20:40:34.985: I neuronx_distributed/trainer/checkpoint.py:144] synced saving of checkpoint adapter_shards began\n",
      "[2025-12-02 20:40:35.139: I neuronx_distributed/trainer/checkpoint.py:192] synced saving of checkpoint adapter_shards completed\n",
      "[2025-12-02 20:40:35.140: I neuronx_distributed/trainer/checkpoint.py:256] no checkpoints to remove.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 0.6279, 'learning_rate': 2.45e-05, 'grad_norm': 0.69921875, 'epoch': 0.1843817787418655}\n",
      "{'loss': 0.5729, 'learning_rate': 2.4e-05, 'grad_norm': 0.59375, 'epoch': 0.18799710773680406}\n",
      "{'loss': 0.5851, 'learning_rate': 2.35e-05, 'grad_norm': 0.703125, 'epoch': 0.19161243673174258}\n",
      "{'loss': 0.6042, 'learning_rate': 2.3000000000000003e-05, 'grad_norm': 0.58984375, 'epoch': 0.19522776572668113}\n",
      "{'loss': 0.5612, 'learning_rate': 2.25e-05, 'grad_norm': 0.5859375, 'epoch': 0.19884309472161968}\n",
      "{'loss': 0.5727, 'learning_rate': 2.2000000000000003e-05, 'grad_norm': 0.7109375, 'epoch': 0.2024584237165582}\n",
      "{'loss': 0.5868, 'learning_rate': 2.15e-05, 'grad_norm': 0.64453125, 'epoch': 0.20607375271149675}\n",
      "{'loss': 0.5552, 'learning_rate': 2.1e-05, 'grad_norm': 0.6484375, 'epoch': 0.2096890817064353}\n",
      "{'loss': 0.5953, 'learning_rate': 2.05e-05, 'grad_norm': 0.63671875, 'epoch': 0.21330441070137382}\n",
      "{'loss': 0.589, 'learning_rate': 2e-05, 'grad_norm': 0.578125, 'epoch': 0.21691973969631237}\n",
      "{'loss': 0.5712, 'learning_rate': 1.9500000000000003e-05, 'grad_norm': 0.62890625, 'epoch': 0.2205350686912509}\n",
      "{'loss': 0.6063, 'learning_rate': 1.9e-05, 'grad_norm': 0.66015625, 'epoch': 0.22415039768618944}\n",
      "{'loss': 0.5976, 'learning_rate': 1.85e-05, 'grad_norm': 0.65234375, 'epoch': 0.227765726681128}\n",
      "{'loss': 0.5681, 'learning_rate': 1.8e-05, 'grad_norm': 0.66796875, 'epoch': 0.2313810556760665}\n",
      "{'loss': 0.5727, 'learning_rate': 1.75e-05, 'grad_norm': 0.56640625, 'epoch': 0.23499638467100506}\n",
      "{'loss': 0.5796, 'learning_rate': 1.7000000000000003e-05, 'grad_norm': 0.77734375, 'epoch': 0.2386117136659436}\n",
      "{'loss': 0.6014, 'learning_rate': 1.65e-05, 'grad_norm': 0.703125, 'epoch': 0.24222704266088213}\n",
      "{'loss': 0.608, 'learning_rate': 1.6000000000000003e-05, 'grad_norm': 0.59765625, 'epoch': 0.24584237165582068}\n",
      "{'loss': 0.5956, 'learning_rate': 1.55e-05, 'grad_norm': 0.76953125, 'epoch': 0.24945770065075923}\n",
      "{'loss': 0.6057, 'learning_rate': 1.5e-05, 'grad_norm': 0.625, 'epoch': 0.2530730296456978}\n",
      "{'loss': 0.583, 'learning_rate': 1.45e-05, 'grad_norm': 0.65625, 'epoch': 0.25668835864063627}\n",
      "{'loss': 0.5922, 'learning_rate': 1.4000000000000001e-05, 'grad_norm': 0.8046875, 'epoch': 0.2603036876355748}\n",
      "{'loss': 0.5784, 'learning_rate': 1.3500000000000001e-05, 'grad_norm': 0.67578125, 'epoch': 0.26391901663051337}\n",
      "{'loss': 0.5647, 'learning_rate': 1.3000000000000001e-05, 'grad_norm': 0.75390625, 'epoch': 0.2675343456254519}\n",
      "{'loss': 0.5591, 'learning_rate': 1.25e-05, 'grad_norm': 0.64453125, 'epoch': 0.27114967462039047}\n",
      "{'loss': 0.5794, 'learning_rate': 1.2e-05, 'grad_norm': 0.62109375, 'epoch': 0.274765003615329}\n",
      "{'loss': 0.5825, 'learning_rate': 1.1500000000000002e-05, 'grad_norm': 0.703125, 'epoch': 0.2783803326102675}\n",
      "{'loss': 0.626, 'learning_rate': 1.1000000000000001e-05, 'grad_norm': 0.828125, 'epoch': 0.28199566160520606}\n",
      "{'loss': 0.5751, 'learning_rate': 1.05e-05, 'grad_norm': 0.58984375, 'epoch': 0.2856109906001446}\n",
      "{'loss': 0.5866, 'learning_rate': 1e-05, 'grad_norm': 0.6328125, 'epoch': 0.28922631959508316}\n",
      "{'loss': 0.543, 'learning_rate': 9.5e-06, 'grad_norm': 0.58984375, 'epoch': 0.2928416485900217}\n",
      "{'loss': 0.5908, 'learning_rate': 9e-06, 'grad_norm': 0.6484375, 'epoch': 0.29645697758496026}\n",
      "{'loss': 0.596, 'learning_rate': 8.500000000000002e-06, 'grad_norm': 0.671875, 'epoch': 0.30007230657989875}\n",
      "{'loss': 0.5656, 'learning_rate': 8.000000000000001e-06, 'grad_norm': 0.69140625, 'epoch': 0.3036876355748373}\n",
      "{'loss': 0.5717, 'learning_rate': 7.5e-06, 'grad_norm': 0.7109375, 'epoch': 0.30730296456977585}\n",
      "{'loss': 0.552, 'learning_rate': 7.000000000000001e-06, 'grad_norm': 0.61328125, 'epoch': 0.3109182935647144}\n",
      "{'loss': 0.5696, 'learning_rate': 6.5000000000000004e-06, 'grad_norm': 0.64453125, 'epoch': 0.31453362255965295}\n",
      "{'loss': 0.5694, 'learning_rate': 6e-06, 'grad_norm': 0.71875, 'epoch': 0.31814895155459144}\n",
      "{'loss': 0.5799, 'learning_rate': 5.500000000000001e-06, 'grad_norm': 0.6484375, 'epoch': 0.32176428054953}\n",
      "{'loss': 0.5572, 'learning_rate': 5e-06, 'grad_norm': 0.70703125, 'epoch': 0.32537960954446854}\n",
      "{'loss': 0.5815, 'learning_rate': 4.5e-06, 'grad_norm': 0.6953125, 'epoch': 0.3289949385394071}\n",
      "{'loss': 0.5654, 'learning_rate': 4.000000000000001e-06, 'grad_norm': 0.65625, 'epoch': 0.33261026753434564}\n",
      "{'loss': 0.5745, 'learning_rate': 3.5000000000000004e-06, 'grad_norm': 0.69140625, 'epoch': 0.3362255965292842}\n",
      "{'loss': 0.572, 'learning_rate': 3e-06, 'grad_norm': 0.75390625, 'epoch': 0.3398409255242227}\n",
      "{'loss': 0.577, 'learning_rate': 2.5e-06, 'grad_norm': 0.62109375, 'epoch': 0.34345625451916123}\n",
      "{'loss': 0.5559, 'learning_rate': 2.0000000000000003e-06, 'grad_norm': 0.66796875, 'epoch': 0.3470715835140998}\n",
      "{'loss': 0.5726, 'learning_rate': 1.5e-06, 'grad_norm': 0.671875, 'epoch': 0.35068691250903833}\n",
      "{'loss': 0.5606, 'learning_rate': 1.0000000000000002e-06, 'grad_norm': 0.640625, 'epoch': 0.3543022415039769}\n",
      "{'loss': 0.6042, 'learning_rate': 5.000000000000001e-07, 'grad_norm': 0.6328125, 'epoch': 0.3579175704989154}\n",
      "{'loss': 0.5545, 'learning_rate': 0.0, 'grad_norm': 0.66796875, 'epoch': 0.3615328994938539}\n",
      "Saving model checkpoint to /home/ubuntu/environment/ml/qwen/checkpoint-1000\n",
      "Model parallelism is enabled, saving the model sharded state dict instead of the full state dict.\n",
      "[2025-12-02 20:49:07.371: I neuronx_distributed/trainer/checkpoint.py:144] synced saving of checkpoint adapter_shards began\n",
      "[2025-12-02 20:49:07.526: I neuronx_distributed/trainer/checkpoint.py:192] synced saving of checkpoint adapter_shards completed\n",
      "[2025-12-02 20:49:07.527: I neuronx_distributed/trainer/checkpoint.py:256] no checkpoints to remove.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1053.6031, 'train_samples_per_second': 1.898, 'train_steps_per_second': 0.949, 'train_loss': 0.6340726699829101, 'epoch': 0.3615328994938539}\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "Consolidating LoRA adapter shards\n",
      "Merging LoRA adapter shards into base model\n",
      "Saving merged model to /home/ubuntu/environment/ml/qwen/merged_model\n",
      "Saving tokenizer to /home/ubuntu/environment/ml/qwen/merged_model\n",
      "Merged model config:\n",
      "Qwen3Model(\n",
      "  (embed_tokens): Embedding(151936, 2048)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen3DecoderLayer(\n",
      "      (self_attn): Qwen3Attention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Qwen3MLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "        (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "  (rotary_emb): Qwen3RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "!torchrun \\\n",
    "  --nnodes 1 \\\n",
    "  --nproc_per_node 2 \\\n",
    "  finetune_model.py \\\n",
    "  --model_id Qwen/Qwen3-1.7B \\\n",
    "  --tokenizer_id Qwen/Qwen3-1.7B \\\n",
    "  --output_dir ~/environment/ml/qwen \\\n",
    "  --bf16 True \\\n",
    "  --gradient_checkpointing True \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --learning_rate 5e-5 \\\n",
    "  --max_steps 1000 \\\n",
    "  --per_device_train_batch_size 2 \\\n",
    "  --tensor_parallel_size 2 \\\n",
    "  --lora_r 16 \\\n",
    "  --lora_alpha 32 \\\n",
    "  --lora_dropout 0.05 \\\n",
    "  --dataloader_drop_last True \\\n",
    "  --disable_tqdm True \\\n",
    "  --logging_steps 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation\n",
    "\n",
    "After completing the fine-tuning process, the next step is to compile the trained model for AWS Trainium inference using the Hugging Face Optimum Neuron toolchain.\n",
    "Neuron compilation optimizes the model graph and converts it into a Neuron Executable File Format (NEFF), enabling efficient execution on NeuronCores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/commands/env.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/commands/env.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/kvcache/kv_cache_manager.py:24: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..attention.gqa import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/llama/modeling_llama.py:45: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "INFO:Neuron:Generating HLOs for the following models: ['context_encoding_model', 'token_generation_model']\n",
      "[2025-12-02 20:54:41.340: I neuronx_distributed/parallel_layers/parallel_state.py:628] > initializing tensor model parallel with size 2\n",
      "[2025-12-02 20:54:41.341: I neuronx_distributed/parallel_layers/parallel_state.py:629] > initializing pipeline model parallel with size 1\n",
      "[2025-12-02 20:54:41.341: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing context model parallel with size 1\n",
      "[2025-12-02 20:54:41.341: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing data parallel with size 1\n",
      "[2025-12-02 20:54:41.341: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing world size to 2\n",
      "[2025-12-02 20:54:41.342: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x71b06c2c49d0>, 'Ascending Ring PG Group')>\n",
      "[2025-12-02 20:54:41.342: I neuronx_distributed/parallel_layers/parallel_state.py:668] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-12-02 20:54:41.342: I neuronx_distributed/parallel_layers/parallel_state.py:669] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-12-02 20:54:41.342: I neuronx_distributed/parallel_layers/parallel_state.py:670] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-12-02 20:54:41.342: I neuronx_distributed/parallel_layers/parallel_state.py:671] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-12-02 20:54:41.342: I neuronx_distributed/parallel_layers/parallel_state.py:672] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-12-02 20:54:41.342: I neuronx_distributed/parallel_layers/parallel_state.py:673] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "INFO:Neuron:Generating 1 hlos for key: context_encoding_model\n",
      "INFO:Neuron:Started loading module context_encoding_model\n",
      "INFO:Neuron:Finished loading module context_encoding_model in 0.138505220413208 seconds\n",
      "INFO:Neuron:generating HLO: context_encoding_model, input example shape = torch.Size([1, 512])\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:299: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs_soft_max, dim=dim, on_cpu=self.on_cpu)\n",
      "Add predicate {{0,+,-1}<i0=[0:128:1]>,+,0}<i1=[0:2048:1]>\n",
      "start lb and ub of  {0,+,-1}<i0=[0:128:1]> is 0 0\n",
      "Add predicate {{255,+,0}<i0=[0:128:1]>,+,-1}<i1=[0:2048:1]>\n",
      "start lb and ub of  {{255,+,0}<i0=[0:128:1]>,+,-1}<i1=[0:2048:1]> is 255 255\n",
      "before build_invert_ranges alive full {\n",
      "  0 <= i1=[0:2048:1] <= 2047; alive full {\n",
      "    0 <= i1=[0:2048:1] <= 2047; 1 <= i0=[0:128:1] <= 127; alive leaf\n",
      "  }\n",
      "  256 <= i1=[0:2048:1] <= 2047; alive {\n",
      "    256 <= i1=[0:2048:1] <= 2047; 0 <= i0=[0:128:1] <= 127; alive full leaf\n",
      "  }\n",
      "}\n",
      "generated domains alive full {\n",
      "  0 <= i1=[0:2048:1] <= 255; alive {\n",
      "    0 <= i1=[0:2048:1] <= 255; 0 <= i0=[0:128:1] <= 0; alive leaf\n",
      "  }\n",
      "}\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:262: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 512]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Finished generating HLO for context_encoding_model in 2.532684564590454 seconds, input example shape = torch.Size([1, 512])\n",
      "INFO:Neuron:Generating 1 hlos for key: token_generation_model\n",
      "INFO:Neuron:Started loading module token_generation_model\n",
      "INFO:Neuron:Finished loading module token_generation_model in 0.1294243335723877 seconds\n",
      "INFO:Neuron:generating HLO: token_generation_model, input example shape = torch.Size([1, 1])\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:299: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs_soft_max, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:262: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Finished generating HLO for token_generation_model in 2.072809934616089 seconds, input example shape = torch.Size([1, 1])\n",
      "INFO:Neuron:Generated all HLOs in 4.936976194381714 seconds\n",
      "INFO:Neuron:Starting compilation for the priority HLO\n",
      "INFO:Neuron:'token_generation_model' is the priority model with bucket rank 0\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_de(â€¦): 100%|â–ˆ| 2.14M/2.14M [00:00<00:00, 5\n",
      "Fetched cached neuronxcc-2.20.9961.0+0acef03a/MODULE_de733c6f96020a1e5f56+a9d440f5/model.neff from aws-neuron/optimum-neuron-cache\n",
      "2025-12-02 20:54:47.000339:  40275  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_de733c6f96020a1e5f56+a9d440f5/model.neff\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_de(â€¦): 100%|â–ˆ| 2.27M/2.27M [00:00<00:00, 4\n",
      "Fetched cached neuronxcc-2.20.9961.0+0acef03a/MODULE_de733c6f96020a1e5f56+a9d440f5/wrapped_neff.hlo from aws-neuron/optimum-neuron-cache\n",
      "INFO:Neuron:Done compilation for the priority HLO in 1.819352149963379 seconds\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Done optimizing weight layout for all HLOs in 0.5134353637695312 seconds\n",
      "INFO:Neuron:Starting compilation for all HLOs\n",
      "INFO:Neuron:Neuron compiler flags: --auto-cast=none --model-type=transformer --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' -O2  --lnc=1 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk0/log-neuron-cc.txt\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:245: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_05(â€¦): 100%|â–ˆ| 3.06M/3.06M [00:00<00:00, 7\n",
      "Fetched cached neuronxcc-2.20.9961.0+0acef03a/MODULE_057bc784fc164fb34d3e+ed72d204/model.neff from aws-neuron/optimum-neuron-cache\n",
      "2025-12-02 20:54:49.000621:  40275  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_057bc784fc164fb34d3e+ed72d204/model.neff\n",
      "INFO:Neuron:Finished Compilation for all HLOs in 1.0004863739013672 seconds\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_e3031c3c114f7c905db7+ae6a382b/model.done not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_e3031c3c114f7c905db7+ae6a382b/model.done not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "..Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Caching neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_e3031c3c114f7c905db7+ae6a382b/model.neff\n",
      "INFO:Neuron:Done preparing weight layout transformation\n",
      "INFO:Neuron:Finished building model in 47.16186022758484 seconds\n",
      "Configuration saved in /home/ubuntu/environment/ml/qwen/compiled_model/neuron_config.json\n",
      "INFO:Neuron:Sharding Weights for ranks: 0...1\n",
      "[2025-12-02 20:55:28.631: I neuronx_distributed/parallel_layers/parallel_state.py:628] > initializing tensor model parallel with size 2\n",
      "[2025-12-02 20:55:28.631: I neuronx_distributed/parallel_layers/parallel_state.py:629] > initializing pipeline model parallel with size 1\n",
      "[2025-12-02 20:55:28.631: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing context model parallel with size 1\n",
      "[2025-12-02 20:55:28.631: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing data parallel with size 1\n",
      "[2025-12-02 20:55:28.631: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing world size to 2\n",
      "[2025-12-02 20:55:28.632: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x71b06c2c49d0>, 'Ascending Ring PG Group')>\n",
      "[2025-12-02 20:55:28.632: I neuronx_distributed/parallel_layers/parallel_state.py:668] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-12-02 20:55:28.632: I neuronx_distributed/parallel_layers/parallel_state.py:669] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-12-02 20:55:28.632: I neuronx_distributed/parallel_layers/parallel_state.py:670] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-12-02 20:55:28.633: I neuronx_distributed/parallel_layers/parallel_state.py:671] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-12-02 20:55:28.633: I neuronx_distributed/parallel_layers/parallel_state.py:672] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-12-02 20:55:28.633: I neuronx_distributed/parallel_layers/parallel_state.py:673] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/trace/trace.py:640: UserWarning: Removing redundant keys from checkpoint: []\n",
      "  warnings.warn(f\"Removing redundant keys from checkpoint: {keys_to_delete}\")\n",
      "INFO:Neuron:Done Sharding weights in 17.682492290012306\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export neuron \\\n",
    "  --model /home/ubuntu/environment/ml/qwen/merged_model \\\n",
    "  --task text-generation \\\n",
    "  --sequence_length 512 \\\n",
    "  --batch_size 1 \\\n",
    "  /home/ubuntu/environment/ml/qwen/compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "We will install the Optimum Neuron vllm library.  Then, run inference using the compiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: optimum-neuron[vllm] in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: transformers~=4.51.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from optimum-neuron[vllm]) (4.51.3)\n",
      "Requirement already satisfied: accelerate==1.8.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from optimum-neuron[vllm]) (1.8.1)\n",
      "Requirement already satisfied: optimum~=1.24.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from optimum-neuron[vllm]) (1.24.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.29.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from optimum-neuron[vllm]) (0.33.4)\n",
      "Requirement already satisfied: numpy<=1.25.2,>=1.22.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from optimum-neuron[vllm]) (1.25.2)\n",
      "Requirement already satisfied: protobuf<4,>=3.20.3 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from optimum-neuron[vllm]) (3.20.3)\n",
      "Collecting vllm==0.9.2 (from optimum-neuron[vllm])\n",
      "  Using cached vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from accelerate==1.8.1->optimum-neuron[vllm]) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from accelerate==1.8.1->optimum-neuron[vllm]) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from accelerate==1.8.1->optimum-neuron[vllm]) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from accelerate==1.8.1->optimum-neuron[vllm]) (2.7.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from accelerate==1.8.1->optimum-neuron[vllm]) (0.5.3)\n",
      "Requirement already satisfied: regex in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (2025.7.34)\n",
      "Collecting cachetools (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: sentencepiece in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (0.2.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (4.67.1)\n",
      "Collecting blake3 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached blake3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting py-cpuinfo (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (0.21.4)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm]) (0.116.1)\n",
      "Requirement already satisfied: aiohttp in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (3.12.15)\n",
      "Collecting openai<=1.90.0,>=1.52.0 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached openai-1.90.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: pydantic>=2.10 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (2.11.7)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (0.22.1)\n",
      "Requirement already satisfied: pillow in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (11.3.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached tiktoken-0.12.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached lm_format_enforcer-0.10.12-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<0.8.0,>=0.7.11 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting outlines==0.1.11 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lark==1.2.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (1.2.2)\n",
      "Collecting xgrammar==0.1.19 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached xgrammar-0.1.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (4.14.1)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (3.18.0)\n",
      "Collecting partial-json-parser (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached partial_json_parser-0.2.1.1.post7-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (27.0.0)\n",
      "Collecting msgspec (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached msgspec-0.20.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting gguf>=0.13.0 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mistral_common>=1.6.2 (from mistral_common[opencv]>=1.6.2->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached mistral_common-1.8.6-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting einops (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.10.2 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached compressed_tensors-0.10.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.18.0 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: cloudpickle in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (3.1.1)\n",
      "Collecting watchfiles (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached watchfiles-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-json-logger in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (3.3.0)\n",
      "Requirement already satisfied: scipy in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (1.12.0)\n",
      "Collecting ninja (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting pybase64 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached pybase64-1.4.2-cp310-cp310-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: numba==0.61.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from vllm==0.9.2->optimum-neuron[vllm]) (0.61.2)\n",
      "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached ray-2.52.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting torch>=2.0.0 (from accelerate==1.8.1->optimum-neuron[vllm])\n",
      "  Using cached torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchaudio==2.7.0 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached torchaudio-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchvision==0.22.0 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers==0.0.30 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting astor (from depyf==0.18.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: dill in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from depyf==0.18.0->vllm==0.9.2->optimum-neuron[vllm]) (0.3.8)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from numba==0.61.2->vllm==0.9.2->optimum-neuron[vllm]) (0.44.0)\n",
      "Collecting interegular (from outlines==0.1.11->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from outlines==0.1.11->vllm==0.9.2->optimum-neuron[vllm]) (3.1.6)\n",
      "Requirement already satisfied: nest_asyncio in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from outlines==0.1.11->vllm==0.9.2->optimum-neuron[vllm]) (1.6.0)\n",
      "Collecting diskcache (from outlines==0.1.11->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: referencing in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from outlines==0.1.11->vllm==0.9.2->optimum-neuron[vllm]) (0.36.2)\n",
      "Requirement already satisfied: jsonschema in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from outlines==0.1.11->vllm==0.9.2->optimum-neuron[vllm]) (4.25.0)\n",
      "Collecting pycountry (from outlines==0.1.11->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting airportsdata (from outlines==0.1.11->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached airportsdata-20250909-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (2.8.8)\n",
      "Requirement already satisfied: fsspec in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (1.11.1.6)\n",
      "Collecting triton==3.3.0 (from torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm])\n",
      "  Using cached triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from triton==3.3.0->torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (80.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from openai<=1.90.0,>=1.52.0->vllm==0.9.2->optimum-neuron[vllm]) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<=1.90.0,>=1.52.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from openai<=1.90.0,>=1.52.0->vllm==0.9.2->optimum-neuron[vllm]) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<=1.90.0,>=1.52.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached jiter-0.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from openai<=1.90.0,>=1.52.0->vllm==0.9.2->optimum-neuron[vllm]) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<=1.90.0,>=1.52.0->vllm==0.9.2->optimum-neuron[vllm]) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<=1.90.0,>=1.52.0->vllm==0.9.2->optimum-neuron[vllm]) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<=1.90.0,>=1.52.0->vllm==0.9.2->optimum-neuron[vllm]) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<=1.90.0,>=1.52.0->vllm==0.9.2->optimum-neuron[vllm]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<=1.90.0,>=1.52.0->vllm==0.9.2->optimum-neuron[vllm]) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from pydantic>=2.10->vllm==0.9.2->optimum-neuron[vllm]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from pydantic>=2.10->vllm==0.9.2->optimum-neuron[vllm]) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from pydantic>=2.10->vllm==0.9.2->optimum-neuron[vllm]) (0.4.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from huggingface-hub>=0.29.0->optimum-neuron[vllm]) (1.1.5)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm]) (0.47.2)\n",
      "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached fastapi_cli-0.0.16-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting typer>=0.15.1 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached rich_toolkit-0.17.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: tomli>=2.0.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm]) (2.2.1)\n",
      "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached fastapi_cloud_cli-0.5.2-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached rignore-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting sentry-sdk>=2.20.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached sentry_sdk-2.46.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting fastar>=0.5.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached fastar-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from jinja2->outlines==0.1.11->vllm==0.9.2->optimum-neuron[vllm]) (3.0.2)\n",
      "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.6.2->mistral_common[opencv]>=1.6.2->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from jsonschema->outlines==0.1.11->vllm==0.9.2->optimum-neuron[vllm]) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from jsonschema->outlines==0.1.11->vllm==0.9.2->optimum-neuron[vllm]) (2025.4.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from jsonschema->outlines==0.1.11->vllm==0.9.2->optimum-neuron[vllm]) (0.26.0)\n",
      "INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: click!=8.3.*,>=7.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm==0.9.2->optimum-neuron[vllm]) (8.2.1)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached msgpack-1.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]!=2.44.*,>=2.43.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached cupy_cuda12x-13.6.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from requests>=2.26.0->vllm==0.9.2->optimum-neuron[vllm]) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from requests>=2.26.0->vllm==0.9.2->optimum-neuron[vllm]) (2.5.0)\n",
      "Requirement already satisfied: rich>=13.7.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm]) (14.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm]) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm]) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.8.1->optimum-neuron[vllm]) (1.3.0)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached httptools-0.7.1-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached uvloop-0.22.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp->vllm==0.9.2->optimum-neuron[vllm]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp->vllm==0.9.2->optimum-neuron[vllm]) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp->vllm==0.9.2->optimum-neuron[vllm]) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp->vllm==0.9.2->optimum-neuron[vllm]) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp->vllm==0.9.2->optimum-neuron[vllm]) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp->vllm==0.9.2->optimum-neuron[vllm]) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from aiohttp->vllm==0.9.2->optimum-neuron[vllm]) (1.20.1)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm==0.9.2->optimum-neuron[vllm])\n",
      "  Using cached fastrlock-0.8.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Using cached vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl (383.4 MB)\n",
      "Using cached compressed_tensors-0.10.2-py3-none-any.whl (169 kB)\n",
      "Using cached depyf-0.18.0-py3-none-any.whl (38 kB)\n",
      "Using cached outlines-0.1.11-py3-none-any.whl (87 kB)\n",
      "Using cached outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "Using cached torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "Using cached torchaudio-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "Using cached torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "Using cached triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
      "Using cached xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl (31.5 MB)\n",
      "Using cached xgrammar-0.1.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "Using cached llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "Using cached lm_format_enforcer-0.10.12-py3-none-any.whl (44 kB)\n",
      "Using cached openai-1.90.0-py3-none-any.whl (734 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jiter-0.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (364 kB)\n",
      "Using cached email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
      "Using cached dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
      "Using cached fastapi_cli-0.0.16-py3-none-any.whl (12 kB)\n",
      "Using cached fastapi_cloud_cli-0.5.2-py3-none-any.whl (23 kB)\n",
      "Using cached fastar-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (821 kB)\n",
      "Using cached gguf-0.17.1-py3-none-any.whl (96 kB)\n",
      "Using cached interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Using cached mistral_common-1.8.6-py3-none-any.whl (6.5 MB)\n",
      "Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Using cached pydantic_extra_types-2.10.6-py3-none-any.whl (40 kB)\n",
      "Using cached pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached ray-2.52.1-cp310-cp310-manylinux2014_x86_64.whl (72.1 MB)\n",
      "Using cached msgpack-1.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (406 kB)\n",
      "Using cached rich_toolkit-0.17.0-py3-none-any.whl (31 kB)\n",
      "Using cached rignore-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (959 kB)\n",
      "Using cached sentry_sdk-2.46.0-py2.py3-none-any.whl (406 kB)\n",
      "Using cached tiktoken-0.12.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "Using cached typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Using cached httptools-0.7.1-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (440 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached uvloop-0.22.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.7 MB)\n",
      "Using cached watchfiles-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (455 kB)\n",
      "Using cached websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "Using cached airportsdata-20250909-py3-none-any.whl (914 kB)\n",
      "Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Using cached blake3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (387 kB)\n",
      "Using cached cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Using cached cupy_cuda12x-13.6.0-cp310-cp310-manylinux2014_x86_64.whl (112.2 MB)\n",
      "Using cached fastrlock-0.8.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached msgspec-0.20.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (222 kB)\n",
      "Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Using cached partial_json_parser-0.2.1.1.post7-py3-none-any.whl (10 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached pybase64-1.4.2-cp310-cp310-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (68 kB)\n",
      "Installing collected packages: py-cpuinfo, fastrlock, websockets, uvloop, uvicorn, triton, shellingham, sentry-sdk, rignore, python-multipart, python-dotenv, pycountry, pybase64, partial-json-parser, opencv-python-headless, ninja, msgspec, msgpack, llguidance, jiter, interegular, httptools, gguf, fastar, einops, dnspython, distro, diskcache, cupy-cuda12x, cachetools, blake3, astor, airportsdata, tiktoken, email-validator, depyf, watchfiles, typer, torch, rich-toolkit, pydantic-extra-types, lm-format-enforcer, xformers, torchvision, torchaudio, ray, prometheus-fastapi-instrumentator, outlines_core, openai, xgrammar, outlines, mistral_common, fastapi-cloud-cli, fastapi-cli, compressed-tensors, vllm\n",
      "\u001b[2K  Attempting uninstall: tritonâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/56\u001b[0m [uvicorn]\n",
      "\u001b[2K    Found existing installation: triton 3.3.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/56\u001b[0m [uvicorn]\n",
      "\u001b[2K    Uninstalling triton-3.3.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/56\u001b[0m [uvicorn]\n",
      "\u001b[2K      Successfully uninstalled triton-3.3.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/56\u001b[0m [uvicorn]\n",
      "\u001b[2K  Attempting uninstall: torchâ”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37/56\u001b[0m [typer]]da12x]-headless]\n",
      "\u001b[2K    Found existing installation: torch 2.7.1\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37/56\u001b[0m [typer]\n",
      "\u001b[2K    Uninstalling torch-2.7.1:â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38/56\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.1[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38/56\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: torchvisionâ”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/56\u001b[0m [xformers]-enforcer]s]\n",
      "\u001b[2K    Found existing installation: torchvision 0.22.10mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42/56\u001b[0m [xformers]\n",
      "\u001b[2K    Uninstalling torchvision-0.22.1:â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43/56\u001b[0m [torchvision]\n",
      "\u001b[2K      Successfully uninstalled torchvision-0.22.1[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43/56\u001b[0m [torchvision]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56/56\u001b[0m [vllm]0m [vllm]0m [compressed-tensors]umentator]\n",
      "\u001b[1A\u001b[2KSuccessfully installed airportsdata-20250909 astor-0.8.1 blake3-1.0.8 cachetools-6.2.2 compressed-tensors-0.10.2 cupy-cuda12x-13.6.0 depyf-0.18.0 diskcache-5.6.3 distro-1.9.0 dnspython-2.8.0 einops-0.8.1 email-validator-2.3.0 fastapi-cli-0.0.16 fastapi-cloud-cli-0.5.2 fastar-0.8.0 fastrlock-0.8.3 gguf-0.17.1 httptools-0.7.1 interegular-0.3.3 jiter-0.12.0 llguidance-0.7.30 lm-format-enforcer-0.10.12 mistral_common-1.8.6 msgpack-1.1.2 msgspec-0.20.0 ninja-1.13.0 openai-1.90.0 opencv-python-headless-4.11.0.86 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post7 prometheus-fastapi-instrumentator-7.1.0 py-cpuinfo-9.0.0 pybase64-1.4.2 pycountry-24.6.1 pydantic-extra-types-2.10.6 python-dotenv-1.2.1 python-multipart-0.0.20 ray-2.52.1 rich-toolkit-0.17.0 rignore-0.7.6 sentry-sdk-2.46.0 shellingham-1.5.4 tiktoken-0.12.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0 triton-3.3.0 typer-0.20.0 uvicorn-0.38.0 uvloop-0.22.1 vllm-0.9.2 watchfiles-1.1.1 websockets-15.0.1 xformers-0.0.30 xgrammar-0.1.19\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optimum-neuron[vllm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-02 20:58:40 [__init__.py:39] Available plugins for group vllm.platform_plugins:\n",
      "INFO 12-02 20:58:40 [__init__.py:41] - optimum_neuron -> optimum.neuron.vllm.plugin:register\n",
      "INFO 12-02 20:58:40 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "INFO:Neuron:Optimum Neuron platform plugin registered for vLLM.\n",
      "INFO:Neuron:Optimum Neuron platform plugin registered for vLLM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-02 20:58:40 [__init__.py:235] Platform plugin optimum_neuron is activated\n",
      "WARNING 12-02 20:58:40 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n",
      "INFO 12-02 20:58:48 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 12-02 20:58:48 [config.py:1472] Using max model len 2048\n",
      "WARNING 12-02 20:58:48 [arg_utils.py:1735] device type=neuron is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 12-02 20:58:49 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='/home/ubuntu/environment/ml/qwen/compiled_model', speculative_config=None, tokenizer='/home/ubuntu/environment/ml/qwen/compiled_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/ubuntu/environment/ml/qwen/compiled_model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":1,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 12-02 20:58:50 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/kvcache/kv_cache_manager.py:24: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..attention.gqa import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/llama/modeling_llama.py:45: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "INFO:Neuron:Loading sharded checkpoint from /home/ubuntu/environment/ml/qwen/compiled_model/checkpoint/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-02 20:58:57 [config.py:4861] Current vLLM config is not set.\n",
      "INFO 12-02 20:58:57 [executor_base.py:113] # neuron blocks: 2, # CPU blocks: 0\n",
      "INFO 12-02 20:58:57 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 2.00x\n",
      "INFO 12-02 20:58:57 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e78aaad6cda444ea30108ff007908c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a568007fa043e49f553ac2eb4e3e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-Dec-02 20:58:57.0815 34728:41274 [0] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):213 CCOM WARN NET/OFI Failed to initialize sendrecv protocol\n",
      "2025-Dec-02 20:58:57.0825 34728:41274 [0] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):354 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2025-Dec-02 20:58:57.0836 34728:41274 [0] ncclResult_t nccl_net_ofi_init_no_atexit_fini_v6(ncclDebugLogger_t):183 CCOM WARN NET/OFI Initializing plugin failed\n",
      "2025-Dec-02 20:58:57.0846 34728:41274 [0] net_plugin.cc:97 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "#########################################################\n",
      "Prompt: '\\n<|im_start|>system\\nYou are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)<|im_end|>\\n<|im_start|>user\\nHow many departments are led by heads who are not mentioned?<|im_end|>\\n<|im_start|>assistant\\n', \n",
      "\n",
      " Generated text: '<think>\\n\\n</think>\\n\\nSELECT COUNT(*) FROM management WHERE NOT department_id = (SELECT department_id FROM department);' \n",
      "\n",
      "Prompt: '\\n<|im_start|>system\\nYou are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR, course_id VARCHAR)<|im_end|>\\n<|im_start|>user\\nWhat are the ids of all students for courses and what are the names of those courses?<|im_end|>\\n<|im_start|>assistant\\n', \n",
      "\n",
      " Generated text: '<think>\\n\\n</think>\\n\\nSELECT T1.course_id, T2.course_name FROM student_course_registrations AS T1 JOIN courses AS T2 ON T1.course_id = T2.course_id GROUP BY T1.student_id;' \n",
      "\n",
      "Prompt: '\\n<|im_start|>system\\nYou are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_9 (wins INTEGER, year VARCHAR, team VARCHAR, points VARCHAR)<|im_end|>\\n<|im_start|>user\\nWhich highest wins number had Kawasaki as a team, 95 points, and a year prior to 1981?<|im_end|>\\n<|im_start|>assistant\\n', \n",
      "\n",
      " Generated text: '<think>\\n\\n</think>\\n\\nSELECT MAX(wins) FROM table_name_9 WHERE points = 95 AND team = \"kawasaki\" AND year < 1981;' \n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(\n",
    "    model=\"/home/ubuntu/environment/ml/qwen/compiled_model\", #local compiled model\n",
    "    max_num_seqs=1,\n",
    "    max_model_len=2048,\n",
    "    device=\"neuron\",\n",
    "    tensor_parallel_size=2,\n",
    "    override_neuron_config={})\n",
    "example1=\"\"\"\n",
    "<|im_start|>system\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)<|im_end|>\n",
    "<|im_start|>user\n",
    "How many departments are led by heads who are not mentioned?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "example2=\"\"\"\n",
    "<|im_start|>system\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR, course_id VARCHAR)<|im_end|>\n",
    "<|im_start|>user\n",
    "What are the ids of all students for courses and what are the names of those courses?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "example3=\"\"\"\n",
    "<|im_start|>system\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE table_name_9 (wins INTEGER, year VARCHAR, team VARCHAR, points VARCHAR)<|im_end|>\n",
    "<|im_start|>user\n",
    "Which highest wins number had Kawasaki as a team, 95 points, and a year prior to 1981?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompts = [\n",
    "    example1,\n",
    "    example2,\n",
    "    example3\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=2048, temperature=0.8)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "print(\"#########################################################\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, \\n\\n Generated text: {generated_text!r} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
